# Bank Marketing Classification - ML Assignment 2

**Student ID:** 2025aa05072  
**Course:** M.Tech (AIML)  
**Institution:** BITS Pilani, Work Integrated Learning Programmes Division

---

## ğŸ“‹ Table of Contents

- [Project Overview](#project-overview)
- [Dataset Information](#dataset-information)
- [Models Implemented](#models-implemented)
- [Installation](#installation)
- [Usage](#usage)
- [Project Structure](#project-structure)
- [Evaluation Results](#evaluation-results)
- [Streamlit Application](#streamlit-application)
- [Deployment](#deployment)
- [Observations & Insights](#observations--insights)
- [Screenshots](#screenshots)
- [References](#references)

---

## ğŸ¯ Project Overview

This project implements an end-to-end machine learning classification pipeline for predicting bank term deposit subscriptions. The system includes:

- **6 Classification Models** trained and evaluated
- **6 Evaluation Metrics** for comprehensive performance analysis
- **Interactive Streamlit Web Application** for model deployment
- **Complete MLOps Pipeline** from data preprocessing to deployment

### Objective

Predict whether a client will subscribe to a term deposit (yes/no) based on demographic and campaign-related features.

---

## ğŸ“Š Dataset Information

### Bank Marketing Dataset

- **Source:** [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/datasets/bank+marketing)
- **Type:** Binary Classification
- **Instances:** 41,188 samples
- **Features:** 20 features (10 numerical, 10 categorical)
- **Target Variable:** `y` (yes/no - will the client subscribe to a term deposit?)
- **Class Distribution:** Imbalanced (majority class: no)

### Features

**Numerical Features (10):**
- `age` - Age of the client
- `duration` - Last contact duration in seconds
- `campaign` - Number of contacts during this campaign
- `pdays` - Days since last contact from previous campaign
- `previous` - Number of contacts before this campaign
- `emp.var.rate` - Employment variation rate
- `cons.price.idx` - Consumer price index
- `cons.conf.idx` - Consumer confidence index
- `euribor3m` - Euribor 3 month rate
- `nr.employed` - Number of employees

**Categorical Features (10):**
- `job` - Type of job
- `marital` - Marital status
- `education` - Education level
- `default` - Has credit in default?
- `housing` - Has housing loan?
- `loan` - Has personal loan?
- `contact` - Contact communication type
- `month` - Last contact month
- `day_of_week` - Last contact day of the week
- `poutcome` - Outcome of previous marketing campaign

---

## ğŸ¤– Models Implemented

### 1. Logistic Regression
- **Type:** Linear classifier
- **Library:** `sklearn.linear_model.LogisticRegression`
- **Hyperparameters:** max_iter=1000, solver='lbfgs'
- **Use Case:** Baseline model, interpretable coefficients

### 2. Decision Tree Classifier
- **Type:** Tree-based classifier
- **Library:** `sklearn.tree.DecisionTreeClassifier`
- **Hyperparameters:** max_depth=10, min_samples_split=20
- **Use Case:** Non-linear decision boundaries, feature importance

### 3. K-Nearest Neighbors (KNN)
- **Type:** Instance-based learning
- **Library:** `sklearn.neighbors.KNeighborsClassifier`
- **Hyperparameters:** n_neighbors=5, weights='uniform'
- **Use Case:** Non-parametric, local decision boundaries

### 4. Naive Bayes (Gaussian)
- **Type:** Probabilistic classifier
- **Library:** `sklearn.naive_bayes.GaussianNB`
- **Hyperparameters:** Default
- **Use Case:** Fast training, probabilistic predictions

### 5. Random Forest
- **Type:** Ensemble (Bagging)
- **Library:** `sklearn.ensemble.RandomForestClassifier`
- **Hyperparameters:** n_estimators=100, max_depth=15, min_samples_split=20
- **Use Case:** Robust to overfitting, feature importance

### 6. XGBoost
- **Type:** Ensemble (Boosting)
- **Library:** `xgboost.XGBClassifier`
- **Hyperparameters:** n_estimators=100, max_depth=6, learning_rate=0.1
- **Use Case:** State-of-the-art performance, handles imbalanced data

---

## ğŸ› ï¸ Installation

### Prerequisites

- Python 3.8 or higher
- pip package manager

### Setup Instructions

1. **Clone the repository**
   ```bash
   git clone https://github.com/jjayakrishnan/2025aa05072_ml_assignment2.git
   cd 2025aa05072_ml_assignment2
   ```

2. **Create virtual environment (recommended)**
   ```bash
   python3 -m venv venv
   source venv/bin/activate  # On Windows: venv\Scripts\activate
   ```

3. **Install dependencies**
   ```bash
   pip install -r requirements.txt
   ```

---

## ğŸš€ Usage

### 1. Train Models

Run the training script to train all 6 models:

```bash
cd model
python train_models.py
```

This will:
- Load and preprocess the Bank Marketing dataset
- Train all 6 classification models
- Evaluate models on test set
- Save trained models to `model/saved_models/`
- Generate evaluation metrics and visualizations

**Expected Output:**
- Trained model files (`.pkl`)
- Model comparison table (`model_comparison.csv`)
- Confusion matrices for each model
- Evaluation results (`evaluation_results.json`)

### 2. Run Streamlit Application

Launch the interactive web application:

```bash
streamlit run app.py
```

The app will open in your browser at `http://localhost:8501`

### 3. Use the Web Application

1. **Select Model:** Choose from 6 trained models
2. **Upload Data:** Upload CSV file with test data
3. **View Results:** See predictions, metrics, and confusion matrix
4. **Compare Models:** Navigate to comparison page to see all model performances

---

## ğŸ“ Project Structure

```
2025aa05072_ml_assignment2/
â”œâ”€â”€ app.py                              # Streamlit web application
â”œâ”€â”€ requirements.txt                    # Python dependencies
â”œâ”€â”€ README.md                          # Project documentation
â”œâ”€â”€ .gitignore                         # Git ignore file
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ bank-additional-full.csv       # Training dataset (41,188 samples)
â”‚   â””â”€â”€ sample_test.csv                # Sample test data for demo
â”‚
â”œâ”€â”€ model/
â”‚   â”œâ”€â”€ preprocessing.py               # Data preprocessing utilities
â”‚   â”œâ”€â”€ train_models.py                # Model training script
â”‚   â”œâ”€â”€ evaluate.py                    # Evaluation metrics utilities
â”‚   â”‚
â”‚   â””â”€â”€ saved_models/                  # Trained models and results
â”‚       â”œâ”€â”€ logistic_regression.pkl    # Logistic Regression model
â”‚       â”œâ”€â”€ decision_tree.pkl          # Decision Tree model
â”‚       â”œâ”€â”€ knn.pkl                    # K-Nearest Neighbors model
â”‚       â”œâ”€â”€ naive_bayes.pkl            # Naive Bayes model
â”‚       â”œâ”€â”€ random_forest.pkl          # Random Forest model
â”‚       â”œâ”€â”€ xgboost.pkl                # XGBoost model
â”‚       â”œâ”€â”€ scaler.pkl                 # Feature scaler
â”‚       â”œâ”€â”€ label_encoders.pkl         # Categorical encoders
â”‚       â”œâ”€â”€ feature_names.pkl          # Feature names
â”‚       â”œâ”€â”€ evaluation_results.json    # Evaluation metrics
â”‚       â”œâ”€â”€ model_comparison.csv       # Model comparison table
â”‚       â”œâ”€â”€ model_comparison.png       # Comparison visualization
â”‚       â”‚
â”‚       â””â”€â”€ confusion_matrices/        # Confusion matrix plots
â”‚           â”œâ”€â”€ logistic_regression_cm.png
â”‚           â”œâ”€â”€ decision_tree_cm.png
â”‚           â”œâ”€â”€ knn_cm.png
â”‚           â”œâ”€â”€ naive_bayes_cm.png
â”‚           â”œâ”€â”€ random_forest_cm.png
â”‚           â””â”€â”€ xgboost_cm.png
â”‚
â””â”€â”€ notebooks/
    â””â”€â”€ exploratory_analysis.ipynb     # (Optional) EDA notebook
```

---

## ğŸ“ˆ Evaluation Results

### Evaluation Metrics

All models are evaluated using 6 comprehensive metrics:

1. **Accuracy** - Overall correctness of predictions
2. **AUC Score** - Area Under the ROC Curve (discrimination ability)
3. **Precision** - Positive Predictive Value (TP / (TP + FP))
4. **Recall** - Sensitivity/True Positive Rate (TP / (TP + FN))
5. **F1 Score** - Harmonic mean of Precision and Recall
6. **MCC** - Matthews Correlation Coefficient (balanced measure)

### Model Performance Comparison

*(Results will be populated after training)*

| Model | Accuracy | AUC Score | Precision | Recall | F1 Score | MCC |
|-------|----------|-----------|-----------|--------|----------|-----|
| Logistic Regression | TBD | TBD | TBD | TBD | TBD | TBD |
| Decision Tree | TBD | TBD | TBD | TBD | TBD | TBD |
| K-Nearest Neighbors | TBD | TBD | TBD | TBD | TBD | TBD |
| Naive Bayes | TBD | TBD | TBD | TBD | TBD | TBD |
| Random Forest | TBD | TBD | TBD | TBD | TBD | TBD |
| XGBoost | TBD | TBD | TBD | TBD | TBD | TBD |

**Best Model:** TBD (based on F1 Score)

---

## ğŸŒ Streamlit Application

### Features

1. **Model Selection**
   - Dropdown menu to select from 6 trained models
   - Dynamic model loading

2. **CSV Upload**
   - Upload custom test data
   - Sample data option for quick testing
   - Data preview with shape information

3. **Predictions & Evaluation**
   - Real-time predictions
   - All 6 evaluation metrics displayed
   - Interactive confusion matrix heatmap
   - Detailed classification report

4. **Model Comparison**
   - Side-by-side performance comparison
   - Visual bar charts
   - Best model highlighting

5. **About Page**
   - Project information
   - Dataset details
   - Model descriptions
   - Usage instructions

### User Interface

- **Clean & Professional Design** with custom CSS styling
- **Responsive Layout** with Streamlit columns
- **Interactive Visualizations** using Matplotlib and Seaborn
- **Color-coded Metrics** for easy interpretation
- **Download Options** for predictions

---

## ğŸš€ Deployment

### GitHub Repository

**URL:** [https://github.com/jjayakrishnan/2025aa05072_ml_assignment2](https://github.com/jjayakrishnan/2025aa05072_ml_assignment2)

### Streamlit Community Cloud

**Live App URL:** TBD (will be updated after deployment)

### Deployment Steps

1. **Push to GitHub**
   ```bash
   git init
   git add .
   git commit -m "Initial commit: ML Assignment 2"
   git branch -M main
   git remote add origin https://github.com/jjayakrishnan/2025aa05072_ml_assignment2.git
   git push -u origin main
   ```

2. **Deploy on Streamlit Cloud**
   - Go to [share.streamlit.io](https://share.streamlit.io)
   - Connect GitHub repository
   - Select `app.py` as main file
   - Click "Deploy"

3. **BITS Virtual Lab Execution**
   - Clone repository in Virtual Lab
   - Install dependencies
   - Run application
   - Capture screenshot for submission

---

## ğŸ” Observations & Insights

### Key Findings

*(To be updated after model training)*

1. **Best Performing Model:**
   - TBD based on F1 Score
   - Reasons for superior performance

2. **Model Comparison:**
   - Ensemble models (Random Forest, XGBoost) typically perform better
   - Logistic Regression provides good baseline
   - Decision Tree may overfit without proper tuning

3. **Dataset Characteristics:**
   - Class imbalance affects model performance
   - Feature engineering opportunities (duration, campaign)
   - Categorical encoding impact

4. **Evaluation Insights:**
   - F1 Score is crucial for imbalanced datasets
   - MCC provides balanced view of performance
   - AUC Score shows discrimination ability

### Recommendations

1. **For Production Use:**
   - Use Random Forest or XGBoost for best performance
   - Consider ensemble of top models
   - Implement threshold tuning for business requirements

2. **Future Improvements:**
   - Hyperparameter tuning (GridSearchCV, RandomizedSearchCV)
   - Feature engineering (interaction terms, polynomial features)
   - Handle class imbalance (SMOTE, class weights)
   - Cross-validation for robust evaluation

---

## ğŸ“¸ Screenshots

*(Screenshots will be added after deployment)*

### 1. Streamlit Application Home Page
![Home Page](screenshots/home_page.png)

### 2. Model Prediction Interface
![Prediction Page](screenshots/prediction_page.png)

### 3. Evaluation Metrics Display
![Metrics](screenshots/metrics_display.png)

### 4. Confusion Matrix
![Confusion Matrix](screenshots/confusion_matrix.png)

### 5. Model Comparison
![Model Comparison](screenshots/model_comparison.png)

### 6. BITS Virtual Lab Execution
![Virtual Lab](screenshots/virtual_lab_execution.png)

---

## ğŸ“š References

1. **Dataset:**
   - [UCI Machine Learning Repository - Bank Marketing](https://archive.ics.uci.edu/ml/datasets/bank+marketing)
   - Moro et al., 2014. "A Data-Driven Approach to Predict the Success of Bank Telemarketing"

2. **Libraries:**
   - [Scikit-learn Documentation](https://scikit-learn.org/)
   - [XGBoost Documentation](https://xgboost.readthedocs.io/)
   - [Streamlit Documentation](https://docs.streamlit.io/)

3. **Metrics:**
   - [Classification Metrics - Scikit-learn](https://scikit-learn.org/stable/modules/model_evaluation.html#classification-metrics)
   - Matthews Correlation Coefficient: [Wikipedia](https://en.wikipedia.org/wiki/Matthews_correlation_coefficient)

---

## ğŸ‘¨â€ğŸ’» Author

**Student ID:** 2025aa05072  
**Program:** M.Tech (AIML)  
**Institution:** BITS Pilani

---

## ğŸ“ License

This project is created for academic purposes as part of ML Assignment 2.

---

## ğŸ™ Acknowledgments

- BITS Pilani for providing the assignment framework
- UCI Machine Learning Repository for the dataset
- Open-source community for excellent ML libraries

---

**Last Updated:** January 2026
